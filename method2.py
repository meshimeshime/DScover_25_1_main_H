# -*- coding: utf-8 -*-
"""김혜수_0625_1D_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D7P_kYPT7MI3-efufO6E5rVYv7aqnvDD
"""

import os, random
# 0) 환경변수로 결정적 연산 강제
os.environ['TF_DETERMINISTIC_OPS'] = '1'

# 1) 파이썬/넘파이/텐서플로 랜덤 시드 고정
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 1) Load data
log_df = pd.read_csv("1m_merged.csv", parse_dates=["timestamp"])
metrics_df = pd.read_csv("ch2025_metrics_train.csv")

# 2) Extract integer subject ID (sid)
log_df['sid'] = log_df['subject_id'].astype(str).str.extract(r'(\d+)', expand=False).astype(int)
metrics_df['sid'] = metrics_df['subject_id'].astype(str).str.extract(r'(\d+)', expand=False).astype(int)

# 3) Prepare date columns
log_df['date'] = log_df['timestamp'].dt.date
metrics_df['lifelog_date'] = pd.to_datetime(metrics_df['lifelog_date']).dt.date

# 4) Match daily sequences
feature_cols = [c for c in log_df.columns if c not in ['timestamp','subject_id','date','sid']]
X_seqs, Y_labels = [], []
for (sid, day), group in log_df.groupby(['sid','date']):
    row = metrics_df[(metrics_df['sid']==sid) & (metrics_df['lifelog_date']==day)]
    if row.shape[0]!=1:
        continue
    seq = group.sort_values('timestamp')[feature_cols].values
    tgt = row[['Q1','Q2','Q3','S1','S2','S3']].iloc[0].values.astype(int)
    X_seqs.append(seq)
    Y_labels.append(tgt)

print("✅ Matched days:", len(X_seqs))

target_cols = ['Q1','Q2','Q3','S1','S2','S3']

# 5) Padding/Truncating to fixed length (1440 minutes)
from tensorflow.keras.preprocessing.sequence import pad_sequences

FIXED_LEN = 1440  # 24h * 60min
X_pad = pad_sequences(
    X_seqs,
    maxlen=FIXED_LEN,
    padding='post',
    truncating='post',
    dtype='float32'
)  # 결과 shape: (450, 1440, F)

X_pad = np.nan_to_num(
    X_pad,
    nan=0.0,
    posinf=0.0,
    neginf=0.0
)
Y = np.vstack(Y_labels)   # shape: (450, 6)

# 6) Scale features
n_samples, seq_len, n_features = X_pad.shape
X_flat = X_pad.reshape(-1, n_features)
scaler = StandardScaler()
X_flat_scaled = scaler.fit_transform(X_flat)
X_scaled = X_flat_scaled.reshape(n_samples, seq_len, n_features)

# 7) Train/Val split
split_idxs = {}
for i, col in enumerate(target_cols):
    y = Y[:, i].astype(int)
    idx = np.arange(len(y))
    train_idx, val_idx = train_test_split(
        idx,
        test_size=0.2,
        random_state=42,
        stratify=y
    )
    split_idxs[col] = (train_idx, val_idx)

import tensorflow as tf

def focal_loss(alpha=0.25, gamma=2.0):
    def loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.math.log(y_pred)
        weight = alpha * tf.math.pow(1 - y_pred, gamma)
        return tf.reduce_sum(weight * cross_entropy, axis=-1)
    return loss_fn

"""1. 기본 모델"""

# 8) 1D CNN 학습 (각 target 별로 독립 모델)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau    # ← 여기 추가
from sklearn.metrics import classification_report, accuracy_score
from sklearn.utils import class_weight


models1 = {}
for i, col in enumerate(target_cols):
    # 8.1) Prepare 1D labels & one-hot
    y = Y[:, i].astype(int)
    num_classes = len(np.unique(y))
    y_cat = to_categorical(y, num_classes)

    # 8.2) Grab split
    train_idx, val_idx = split_idxs[col]
    X_train = X_scaled[train_idx]
    X_val   = X_scaled[val_idx]
    y_train = y_cat[train_idx]
    y_val   = y_cat[val_idx]

    # 8.3) Cast all to float32 to avoid dtype issues
    X_train = X_train.astype('float32')
    X_val   = X_val.astype('float32')
    y_train = y_train.astype('float32')
    y_val   = y_val.astype('float32')

    # 8.4) Compute class weights
    y_train_int = np.argmax(y_train, axis=1)    # one-hot → integer
    weights = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_train_int
    )
    cw = dict(enumerate(weights))


    # 8.5) Build model
    model = Sequential([
        Conv1D(64, 3, activation='relu', input_shape=(seq_len, n_features)),
        MaxPooling1D(2), Dropout(0.3),
        Conv1D(128, 3, activation='relu'), MaxPooling1D(2), Dropout(0.3),
        Flatten(), Dense(64, activation='relu'), Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss=focal_loss(alpha=0.25, gamma=2.0),
        metrics=['accuracy']
    )

    # 8.6) Train with class weights
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        class_weight=cw,            # 8.4 에서 만든 cw
        epochs=50, batch_size=32,
        callbacks=cbs,
        verbose=1
    )

    # 8.7) Quick check
    vp = np.argmax(model.predict(X_val), axis=1)
    vt = np.argmax(y_val, axis=1)
    print(f"\n--- {col} Val Acc: {accuracy_score(vt, vp):.4f} ---")
    print(classification_report(vt, vp))

    # threshold 튜닝 코드도 이 자리(8.7)에서 실행하세요:
    # 이진 분류에만 threshold tuning
    if num_classes == 2:
        probs = model.predict(X_val)[:,1]   # 클래스1 확률
        from sklearn.metrics import f1_score
        best_f1, best_thr = 0, 0.5
        for thr in np.linspace(0.1,0.9,17):
            preds = (probs>thr).astype(int)
            f1 = f1_score(vt, preds)  # 이진이니 default OK
            if f1 > best_f1:
                best_f1, best_thr = f1, thr
        print(f"{col} 최적 임계치: {best_thr:.2f}, 최적 F1: {best_f1:.3f}")
    else:
        # 멀티클래스일 땐 macro F1 한 번만 찍어 보기
        from sklearn.metrics import f1_score
        preds = np.argmax(model.predict(X_val), axis=1)
        f1m = f1_score(vt, preds, average='macro')
        print(f"{col} 멀티클래스 macro F1: {f1m:.3f}")


    models1[col] = model

    # Evaluate
    train_pred = np.argmax(model.predict(X_train), axis=1)
    val_pred   = np.argmax(model.predict(X_val),   axis=1)
    train_true = np.argmax(y_train, axis=1)
    val_true   = np.argmax(y_val,   axis=1)

    print(f"\n--- {col} ---")
    print(f"Train Acc: {accuracy_score(train_true, train_pred):.4f}")
    print(f"Val   Acc: {accuracy_score(val_true,   val_pred):.4f}")
    print(classification_report(val_true, val_pred))



"""2. 기본 argmax 예측 vs. 최적 임계치(best_thr) 적용 예측"""

# 8) 1D CNN 학습 (각 target 별로 독립 모델)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau    # ← 여기 추가
from sklearn.metrics import classification_report, accuracy_score
from sklearn.utils import class_weight


models2 = {}
for i, col in enumerate(target_cols):
    # 8.1) Prepare 1D labels & one-hot
    y = Y[:, i].astype(int)
    num_classes = len(np.unique(y))
    y_cat = to_categorical(y, num_classes)

    # 8.2) Grab split
    train_idx, val_idx = split_idxs[col]
    X_train = X_scaled[train_idx]
    X_val   = X_scaled[val_idx]
    y_train = y_cat[train_idx]
    y_val   = y_cat[val_idx]

    # 8.3) Cast all to float32 to avoid dtype issues
    X_train = X_train.astype('float32')
    X_val   = X_val.astype('float32')
    y_train = y_train.astype('float32')
    y_val   = y_val.astype('float32')

    # 8.4) Compute class weights
    y_train_int = np.argmax(y_train, axis=1)    # one-hot → integer
    weights = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_train_int
    )
    cw = dict(enumerate(weights))


    # 8.5) Build model
    model = Sequential([
        Conv1D(64, 3, activation='relu', input_shape=(seq_len, n_features)),
        MaxPooling1D(2), Dropout(0.3),
        Conv1D(128, 3, activation='relu'), MaxPooling1D(2), Dropout(0.3),
        Flatten(), Dense(64, activation='relu'), Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss=focal_loss(alpha=0.25, gamma=2.0),
        metrics=['accuracy']
    )

    # 8.6) Train with class weights
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        class_weight=cw,            # 8.4 에서 만든 cw
        epochs=50, batch_size=32,
        callbacks=cbs,
        verbose=1
    )

    # 8.7) Quick check
    vp = np.argmax(model.predict(X_val), axis=1)
    vt = np.argmax(y_val, axis=1)
    print(f"\n--- {col} Val Acc: {accuracy_score(vt, vp):.4f} ---")
    print(classification_report(vt, vp))

    # threshold 튜닝 코드도 이 자리(8.7)에서 실행하세요:
    # 이진 분류에만 threshold tuning
    if num_classes == 2:
        probs = model.predict(X_val)[:,1]   # 클래스1 확률
        from sklearn.metrics import f1_score
        best_f1, best_thr = 0, 0.5
        for thr in np.linspace(0.1,0.9,17):
            preds = (probs>thr).astype(int)
            f1 = f1_score(vt, preds)  # 이진이니 default OK
            if f1 > best_f1:
                best_f1, best_thr = f1, thr
        print(f"{col} 최적 임계치: {best_thr:.2f}, 최적 F1: {best_f1:.3f}")
    else:
        # 멀티클래스일 땐 macro F1 한 번만 찍어 보기
        from sklearn.metrics import f1_score
        preds = np.argmax(model.predict(X_val), axis=1)
        f1m = f1_score(vt, preds, average='macro')
        print(f"{col} 멀티클래스 macro F1: {f1m:.3f}")

    # ── 8.8) 임계치 적용 성능 비교 ─────────────────────────
    if num_classes == 2:
        # best_thr 로 예측
        vp_thr = (model.predict(X_val)[:,1] > best_thr).astype(int)
        print(f"--- {col} threshold={best_thr:.2f} 적용 Val Acc: "
              f"{accuracy_score(vt, vp_thr):.4f} ---")
        print(classification_report(vt, vp_thr))
    # ─────────────────────────────────────────────────────



    models2[col] = model

    # Evaluate
    train_pred = np.argmax(model.predict(X_train), axis=1)
    val_pred   = np.argmax(model.predict(X_val),   axis=1)
    train_true = np.argmax(y_train, axis=1)
    val_true   = np.argmax(y_val,   axis=1)

    print(f"\n--- {col} ---")
    print(f"Train Acc: {accuracy_score(train_true, train_pred):.4f}")
    print(f"Val   Acc: {accuracy_score(val_true,   val_pred):.4f}")
    print(classification_report(val_true, val_pred))

"""3. 보완 모델

클래스 불균형 보정 (class_weight)
훈련 시 class_weight='balanced' 로 minority 클래스에 더 무거운 손실 가중치를 주도록 했습니다.
(→ 이전처럼 직접 오버샘플링은 하지 않고, 손실 가중치만 사용)

시계열 Jitter 증강
소수 클래스에 노이즈를 살짝 섞어 데이터 수를 늘려 주는 augment_jitter 함수만 남겼습니다.
(→ 이전에 썼던 RandomOverSampler, MixUp/CutMix 등 무거운 증강은 모두 제거)

모델 구조 강화

BatchNormalization 으로 내부 공변량 이동 완화

GlobalAveragePooling1D 으로 파라미터 수 줄이면서 요약적인 시퀀스 표현 획득

드롭아웃 비율을 단계별로 달리 적용
(→ 단순 두 개의 Conv1D+Flatten 모델 대신 좀 더 깊이 있고 안정적인 구조)

Loss 함수 간소화

이진 분류는 binary_crossentropy

다중 분류는 categorical_crossentropy
(→ 성능이 들쑥날쑥했던 focal_loss는 제거)

학습 안정화 콜백

EarlyStopping (patience=5)

ReduceLROnPlateau (patience=3, factor=0.5)
(→ 일정 에포크 이상 개선 없으면 멈추고, 학습률도 자동으로 낮춤)

Macro-F1 최적화를 위한 Threshold 튜닝
이진 타깃에 한해, Val set에서 0.1~0.9 사이를 훑어 macro-F1이 가장 높은 임계치(thr)를 찾고,
그 임계치와 기본 argmax(.5) 중 macro-F1이 더 높은 쪽을 최종 예측에 사용합니다.
"""

from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
import numpy as np

# ------------------------------------------
# (0) 시계열 Jitter 증강 함수
def augment_jitter(X, sigma=0.02):
    noise = np.random.normal(loc=0.0, scale=sigma, size=X.shape)
    return X + noise
# ------------------------------------------

models3 = {}
for i, col in enumerate(target_cols):
    print(f"\n===== Training for target: {col} =====")

    # (1) 레이블 준비
    y_int      = Y[:, i].astype(int)
    num_classes= len(np.unique(y_int))
    y_cat      = to_categorical(y_int, num_classes)

    # (2) Train/Val split
    train_idx, val_idx = split_idxs[col]
    X_train = X_scaled[train_idx].astype('float32')
    X_val   = X_scaled[val_idx].astype('float32')
    y_train = y_cat[ train_idx].astype('float32')
    y_val   = y_cat[ val_idx].astype('float32')

    # (3) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[train_idx]
    )
    cw = dict(enumerate(cw_vals))

    # (4) Jitter 증강
    X_aug = augment_jitter(X_train, sigma=0.02)
    y_aug = y_train.copy()
    X_train = np.concatenate([X_train, X_aug], axis=0)
    y_train = np.concatenate([y_train, y_aug], axis=0)

    # (5) 모델 정의
    model = Sequential([
        Input(shape=(seq_len, n_features)),
        Conv1D(64, 3, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.3),

        Conv1D(128, 5, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.4),

        Conv1D(256, 3, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.5),

        GlobalAveragePooling1D(),
        Dense(128, activation='relu'), Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    loss_fn = 'binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy'
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

    # (6) 콜백
    callbacks = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # (7) 학습
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # (8) 평가 및 macro-F1 기반 임계치 선택
    vt        = np.argmax(y_val, axis=1)
    probs_all = model.predict(X_val)                  # shape (N, num_classes)

    # 8.1) argmax(.5) 기준
    vp_arg = np.argmax(probs_all, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax(.5) macro-F1: {f1_arg:.4f}")

    # 8.2) 이진 분류의 경우 threshold 튜닝
    if num_classes == 2:
        best_f1, best_thr = f1_arg, 0.5
        for thr in np.linspace(0.1, 0.9, 17):
            vp_thr = (probs_all[:,1] > thr).astype(int)
            f1_t = f1_score(vt, vp_thr, average='macro')
            if f1_t > best_f1:
                best_f1, best_thr = f1_t, thr

        print(f"{col} best-threshold={best_thr:.2f}, macro-F1: {best_f1:.4f}")

        # 최종 예측 선택
        if best_f1 > f1_arg:
            vp_final = (probs_all[:,1] > best_thr).astype(int)
            print(f"=> {col}: threshold={best_thr:.2f} 적용")
        else:
            vp_final = vp_arg
            print(f"=> {col}: argmax(.5) 유지")

    else:
        # 멀티클래스는 argmax 고정
        vp_final = vp_arg
        f1_m = f1_score(vt, vp_final, average='macro')
        print(f"{col} multiclass macro-F1: {f1_m:.4f}")

    # 8.3) 최종 리포트
    print(f"--- {col} 최종 성능 ---")
    print(classification_report(vt, vp_final, zero_division=0))

    models3[col] = model

"""제출파일 생성"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ————— 1) sample 불러오기 & 전처리 —————
submission = pd.read_csv('ch2025_submission_sample.csv')

# subject_id 에서 숫자만 추출해서 sid 로
submission['sid'] = (
    submission['subject_id']
      .astype(str)
      .str.extract(r'(\d+)', expand=False)
      .astype(int)
)

# 날짜 컬럼을 datetime.date 로
submission['lifelog_date'] = pd.to_datetime(submission['lifelog_date']).dt.date
submission['sleep_date']   = pd.to_datetime(submission['sleep_date']).dt.date

# ————— 2) 시퀀스 뽑아서 keys 에 담기 —————
submit_seqs = []
keys = []
for _, row in submission.iterrows():
    sid          = row['sid']
    lifelog_date = row['lifelog_date']
    sleep_date   = row['sleep_date']

    grp = log_df[
        (log_df['sid'] == sid) &
        (log_df['date'] == lifelog_date)
    ]
    if grp.empty:
        # 로그가 없으면 전부 0으로 예측
        submit_seqs.append(np.zeros((FIXED_LEN, len(feature_cols)), dtype='float32'))
        keys.append((row['subject_id'], sleep_date, lifelog_date))
        continue

    seq = grp.sort_values('timestamp')[feature_cols].values
    submit_seqs.append(seq)
    keys.append((row['subject_id'], sleep_date, lifelog_date))

# ————— 3) Padding & Scaling —————
FIXED_LEN = 1440  # 하루 길이
# 3-1) pad & NaN→0
X_sub = pad_sequences(
    submit_seqs,
    maxlen=FIXED_LEN,
    padding='post',
    truncating='post',
    dtype='float32'
)
X_sub = np.nan_to_num(X_sub, nan=0., posinf=0., neginf=0.)

# 3-2) flat → scale → reshape
n_features   = len(feature_cols)
seq_len      = X_sub.shape[1]
X_flat       = X_sub.reshape(-1, n_features)
X_flat       = scaler.transform(X_flat)
X_sub_scaled = X_flat.reshape(-1, seq_len, n_features)

# ————— 4) 예측 & 저장 —————
subm = pd.DataFrame(keys, columns=['subject_id','sleep_date','lifelog_date'])

# 학습 시 찾은 최적 임계치 (예시 값: 실제 학습 로그에서 가져오세요)
best_thresholds = {
    'Q1': 0.60,
    'Q2': 0.35,
    'Q3': 0.50,
    'S2': 0.55,
    'S3': 0.60
}

for col in target_cols:
    preds = models[col].predict(X_sub_scaled)  # shape (N, num_classes)

    # 이진타깃이면 threshold 적용
    if col in best_thresholds:
        thr = best_thresholds[col]
        # 클래스1 확률이 thr 보다 크면 1, 아니면 0
        subm[col] = (preds[:,1] > thr).astype(int)
    else:
        # 다중 클래스(여기선 S1)는 argmax
        subm[col] = np.argmax(preds, axis=1)

# 컬럼 순서 딱 맞추기
subm = subm[['subject_id','sleep_date','lifelog_date'] + target_cols]

subm.to_csv('0624_tuning_submission.csv', index=False)
print("✅ 0624_submission.csv 생성 완료")
print(subm.head())

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Val 인덱스와 모델, 그리고 Y 배열에서 S1만 뽑기
train_idx_s1, val_idx_s1 = split_idxs['S1']      # S1 용 train/val split
X_val_s1 = X_scaled[val_idx_s1]                  # Val 입력
# one-hot 이 아닌 정수 레이블
vt_s1 = Y[val_idx_s1, target_cols.index('S1')]   # Y 는 (N,6) 배열, index로 S1 위치

# 2) 모델로 예측
probs_s1 = models['S1'].predict(X_val_s1)        # (N,3) 확률
vp_s1 = np.argmax(probs_s1, axis=1)              # 정수 예측

# 3) Confusion Matrix 계산 & 출력
cm = confusion_matrix(vt_s1, vp_s1, labels=[0,1,2])
print("S1 Confusion Matrix:\n", cm)

# 4) 히트맵으로 시각화 (optional)
plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[0,1,2], yticklabels=[0,1,2])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('S1 Confusion Matrix')
plt.show()

"""4. 스무트, 멀티 클래스 focal loss

A) SMOTEENN을 이용한 오버/언더샘플링과 B) 멀티클래스 Focal Loss를 S1에만 적용하도록 한 전체 모델링 루프 예시입니다. 다른 타깃들은 기존 binary_crossentropy/categorical_crossentropy + class_weight + jitter + BatchNorm + GlobalAvgPool 구조를 그대로 사용
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Flatten, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
from imblearn.combine import SMOTEENN

# 0) Optional: 아주 약한 Jitter 함수
def augment_jitter(X, sigma=0.01):
    return X + np.random.normal(0, sigma, X.shape)

# 1) S1 전용 multiclass focal loss
def multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0):
    alpha = tf.constant(alpha, dtype=tf.float32)
    def loss_fn(y_true, y_pred):
        ce     = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        p_t    = tf.reduce_sum(y_true * y_pred, axis=-1)
        alpha_t= tf.reduce_sum(y_true * alpha, axis=-1)
        return alpha_t * tf.pow(1. - p_t, gamma) * ce
    return loss_fn

models4 = {}
for i, col in enumerate(target_cols):
    print(f"\n=== Training {col} ===")
    # a) 레이블 & split 준비
    y_int       = Y[:, i].astype(int)
    num_classes = len(np.unique(y_int))
    y_cat       = to_categorical(y_int, num_classes)
    tr_idx, va_idx = split_idxs[col]
    X_tr = X_scaled[tr_idx].astype('float32')
    X_va = X_scaled[va_idx].astype('float32')
    y_tr = y_cat[tr_idx].astype('float32')
    y_va = y_cat[va_idx].astype('float32')

    # b) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[tr_idx]
    )
    cw = dict(enumerate(cw_vals))

    # 분기: S1 에만 특별처리
    if col == 'S1':
        # 2-A) 원본 + 약한 Jitter → SMOTEENN
        X_aug = augment_jitter(X_tr, sigma=0.01)
        y_aug = y_tr.copy()
        X_tmp = np.vstack([X_tr, X_aug])
        y_tmp = np.vstack([y_tr, y_aug])
        # SMOTEENN → 오버/언더샘플링
        sm = SMOTEENN(random_state=42)
        ns, sl, nf = X_tmp.shape
        Xf = X_tmp.reshape(ns, -1)
        yr = np.argmax(y_tmp, axis=1)
        X_res, y_res = sm.fit_resample(Xf, yr)
        X_tr = X_res.reshape(-1, sl, nf).astype('float32')
        y_tr = to_categorical(y_res, num_classes).astype('float32')

        # 2-B) Advanced CNN + focal loss
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 5, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.4),
            Conv1D(256, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.5),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'), Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0)

    else:
        # 다른 타깃: 원래 CNN 구조 + BCE/CategoricalCE
        # (원하면 약한 jitter만 적용)
        # X_tr = augment_jitter(X_tr, sigma=0.01)  # 선택사항
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Flatten(),
            Dense(64, activation='relu'), Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'

    # c) 컴파일 & 콜백
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # d) 학습
    model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=cbs, verbose=1
    )

    # e) 평가 & threshold 튜닝 (macro-F1)
    vt = np.argmax(y_va, axis=1)
    probs = model.predict(X_va)
    vp_arg = np.argmax(probs, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax F1(macro): {f1_arg:.4f}")

    if num_classes==2:
        best_f1, best_thr = f1_arg, 0.5
        for thr in np.linspace(0.1,0.9,17):
            vp_thr = (probs[:,1] > thr).astype(int)
            f1_t = f1_score(vt, vp_thr, average='macro')
            if f1_t>best_f1:
                best_f1, best_thr = f1_t, thr
        print(f"{col} best-thr={best_thr:.2f}, F1: {best_f1:.4f}")
        vp_final = (probs[:,1] > best_thr).astype(int) if best_f1>f1_arg else vp_arg
    else:
        vp_final = vp_arg
        print(f"{col} multiclass F1(macro): {f1_score(vt, vp_final, average='macro'):.4f}")

    print(classification_report(vt, vp_final, zero_division=0))
    models4[col] = model

"""5. 지금까지 성능 확인한거 기반으로 평가요소별 모델 달리

 S1, S3에만 SMOTEENN 기반 샘플링을 적용하고,

S1 에만 multiclass focal loss

S3 및 나머지 (Q1/Q2/Q3/S2) 에는 binary/categorical crossentropy

그리고 나머지 에는 원래 단순 CNN + class_weight + jitter + BatchNorm+GlobalAvgPool + threshold 튜닝

평가 요소 별로 다른 모델 적용
"""



"""5_1. S1,3에 최적화된 모델"""



import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Flatten, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
from imblearn.combine import SMOTEENN

# 0) Optional: 아주 약한 Jitter 함수
def augment_jitter(X, sigma=0.01):
    return X + np.random.normal(0, sigma, X.shape)

# 1) S1 전용 multiclass focal loss
def multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0):
    alpha = tf.constant(alpha, dtype=tf.float32)
    def loss_fn(y_true, y_pred):
        ce     = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        p_t    = tf.reduce_sum(y_true * y_pred, axis=-1)
        alpha_t= tf.reduce_sum(y_true * alpha, axis=-1)
        return alpha_t * tf.pow(1. - p_t, gamma) * ce
    return loss_fn

models5_1 = {}
for col in ['S1','S3']:
    print(f"\n=== Training {col} ===")
    # a) 레이블 & split 준비
    idx         = target_cols.index(col)
    y_int       = Y[:, idx].astype(int)
    num_classes = len(np.unique(y_int))
    y_cat       = to_categorical(y_int, num_classes)
    tr_idx, va_idx = split_idxs[col]
    X_tr = X_scaled[tr_idx].astype('float32')
    X_va = X_scaled[va_idx].astype('float32')
    y_tr = y_cat[tr_idx].astype('float32')
    y_va = y_cat[va_idx].astype('float32')

    # b) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[tr_idx]
    )
    cw = dict(enumerate(cw_vals))

    # 분기: S1 에만 특별처리
    if col == 'S1':
        # 2-A) 원본 + 약한 Jitter → SMOTEENN
        X_aug = augment_jitter(X_tr, sigma=0.01)
        y_aug = y_tr.copy()
        X_tmp = np.vstack([X_tr, X_aug])
        y_tmp = np.vstack([y_tr, y_aug])
        # SMOTEENN → 오버/언더샘플링
        sm = SMOTEENN(random_state=42)
        ns, sl, nf = X_tmp.shape
        Xf = X_tmp.reshape(ns, -1)
        yr = np.argmax(y_tmp, axis=1)
        X_res, y_res = sm.fit_resample(Xf, yr)
        X_tr = X_res.reshape(-1, sl, nf).astype('float32')
        y_tr = to_categorical(y_res, num_classes).astype('float32')

        # 2-B) Advanced CNN + focal loss
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 5, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.4),
            Conv1D(256, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.5),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'), Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0)

    else:
        # 다른 타깃: 원래 CNN 구조 + BCE/CategoricalCE
        # (원하면 약한 jitter만 적용)
        # X_tr = augment_jitter(X_tr, sigma=0.01)  # 선택사항
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Flatten(),
            Dense(64, activation='relu'), Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'

    # c) 컴파일 & 콜백
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # d) 학습
    model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=cbs, verbose=1
    )

    # e) 평가 & threshold 튜닝 (macro-F1)
    vt = np.argmax(y_va, axis=1)
    probs = model.predict(X_va)
    vp_arg = np.argmax(probs, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax F1(macro): {f1_arg:.4f}")

    if num_classes==2:
        best_f1, best_thr = f1_arg, 0.5
        for thr in np.linspace(0.1,0.9,17):
            vp_thr = (probs[:,1] > thr).astype(int)
            f1_t = f1_score(vt, vp_thr, average='macro')
            if f1_t>best_f1:
                best_f1, best_thr = f1_t, thr
        print(f"{col} best-thr={best_thr:.2f}, F1: {best_f1:.4f}")
        vp_final = (probs[:,1] > best_thr).astype(int) if best_f1>f1_arg else vp_arg
    else:
        vp_final = vp_arg
        print(f"{col} multiclass F1(macro): {f1_score(vt, vp_final, average='macro'):.4f}")

    print(classification_report(vt, vp_final, zero_division=0))
    models5_1[col] = model

"""5_2. S1에 최적화된 모델"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Flatten, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
from imblearn.combine import SMOTEENN

# 0) Optional: 아주 약한 Jitter 함수
def augment_jitter(X, sigma=0.01):
    return X + np.random.normal(0, sigma, X.shape)

# 1) S1 전용 multiclass focal loss
def multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0):
    alpha = tf.constant(alpha, dtype=tf.float32)
    def loss_fn(y_true, y_pred):
        ce     = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        p_t    = tf.reduce_sum(y_true * y_pred, axis=-1)
        alpha_t= tf.reduce_sum(y_true * alpha, axis=-1)
        return alpha_t * tf.pow(1. - p_t, gamma) * ce
    return loss_fn

models5_2 = {}
for col in ['S1']:
    print(f"\n=== Training {col} ===")
    # a) 레이블 & split 준비
    idx         = target_cols.index(col)
    y_int       = Y[:, idx].astype(int)
    num_classes = len(np.unique(y_int))
    y_cat       = to_categorical(y_int, num_classes)
    tr_idx, va_idx = split_idxs[col]
    X_tr = X_scaled[tr_idx].astype('float32')
    X_va = X_scaled[va_idx].astype('float32')
    y_tr = y_cat[tr_idx].astype('float32')
    y_va = y_cat[va_idx].astype('float32')

    # b) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[tr_idx]
    )
    cw = dict(enumerate(cw_vals))

    # 분기: S1 에만 특별처리
    if col == 'S1':
        # 2-A) 원본 + 약한 Jitter → SMOTEENN
        X_aug = augment_jitter(X_tr, sigma=0.01)
        y_aug = y_tr.copy()
        X_tmp = np.vstack([X_tr, X_aug])
        y_tmp = np.vstack([y_tr, y_aug])
        # SMOTEENN → 오버/언더샘플링
        sm = SMOTEENN(random_state=42)
        ns, sl, nf = X_tmp.shape
        Xf = X_tmp.reshape(ns, -1)
        yr = np.argmax(y_tmp, axis=1)
        X_res, y_res = sm.fit_resample(Xf, yr)
        X_tr = X_res.reshape(-1, sl, nf).astype('float32')
        y_tr = to_categorical(y_res, num_classes).astype('float32')

        # 2-B) Advanced CNN + focal loss
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 5, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.4),
            Conv1D(256, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.5),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'), Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0)

    else:
        # 다른 타깃: 원래 CNN 구조 + BCE/CategoricalCE
        # (원하면 약한 jitter만 적용)
        # X_tr = augment_jitter(X_tr, sigma=0.01)  # 선택사항
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Flatten(),
            Dense(64, activation='relu'), Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'

    # c) 컴파일 & 콜백
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # d) 학습
    model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=cbs, verbose=1
    )

    # e) 평가 & threshold 튜닝 (macro-F1)
    vt = np.argmax(y_va, axis=1)
    probs = model.predict(X_va)
    vp_arg = np.argmax(probs, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax F1(macro): {f1_arg:.4f}")

    if num_classes==2:
        best_f1, best_thr = f1_arg, 0.5
        for thr in np.linspace(0.1,0.9,17):
            vp_thr = (probs[:,1] > thr).astype(int)
            f1_t = f1_score(vt, vp_thr, average='macro')
            if f1_t>best_f1:
                best_f1, best_thr = f1_t, thr
        print(f"{col} best-thr={best_thr:.2f}, F1: {best_f1:.4f}")
        vp_final = (probs[:,1] > best_thr).astype(int) if best_f1>f1_arg else vp_arg
    else:
        vp_final = vp_arg
        print(f"{col} multiclass F1(macro): {f1_score(vt, vp_final, average='macro'):.4f}")

    print(classification_report(vt, vp_final, zero_division=0))
    models5_2[col] = model

"""models5_special. S3에 최적화"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Flatten, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
from imblearn.combine import SMOTEENN

# 0) Optional: 아주 약한 Jitter 함수
def augment_jitter(X, sigma=0.01):
    return X + np.random.normal(0, sigma, X.shape)

# 1) S1 전용 multiclass focal loss
def multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0):
    alpha = tf.constant(alpha, dtype=tf.float32)
    def loss_fn(y_true, y_pred):
        ce     = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        p_t    = tf.reduce_sum(y_true * y_pred, axis=-1)
        alpha_t= tf.reduce_sum(y_true * alpha, axis=-1)
        return alpha_t * tf.pow(1. - p_t, gamma) * ce
    return loss_fn

models5_special = {}
for col in ['S3']:
    print(f"\n=== Training {col} ===")
    # a) 레이블 & split 준비
    idx         = target_cols.index(col)
    y_int       = Y[:, idx].astype(int)
    num_classes = len(np.unique(y_int))
    y_cat       = to_categorical(y_int, num_classes)
    tr_idx, va_idx = split_idxs[col]
    X_tr = X_scaled[tr_idx].astype('float32')
    X_va = X_scaled[va_idx].astype('float32')
    y_tr = y_cat[tr_idx].astype('float32')
    y_va = y_cat[va_idx].astype('float32')

    # b) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[tr_idx]
    )
    cw = dict(enumerate(cw_vals))

    # 분기: S1 에만 특별처리
    if col == 'S1':
        # 2-A) 원본 + 약한 Jitter → SMOTEENN
        X_aug = augment_jitter(X_tr, sigma=0.01)
        y_aug = y_tr.copy()
        X_tmp = np.vstack([X_tr, X_aug])
        y_tmp = np.vstack([y_tr, y_aug])
        # SMOTEENN → 오버/언더샘플링
        sm = SMOTEENN(random_state=42)
        ns, sl, nf = X_tmp.shape
        Xf = X_tmp.reshape(ns, -1)
        yr = np.argmax(y_tmp, axis=1)
        X_res, y_res = sm.fit_resample(Xf, yr)
        X_tr = X_res.reshape(-1, sl, nf).astype('float32')
        y_tr = to_categorical(y_res, num_classes).astype('float32')

        # 2-B) Advanced CNN + focal loss
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 5, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.4),
            Conv1D(256, 3, activation='relu'),
            BatchNormalization(), MaxPooling1D(2), Dropout(0.5),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'), Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = multiclass_focal_loss(alpha=[0.2,0.5,0.3], gamma=2.0)

    else:
        # 다른 타깃: 원래 CNN 구조 + BCE/CategoricalCE
        X_tr = np.concatenate([X_tr, augment_jitter(X_tr, sigma=0.01)], axis=0)
        y_tr = np.concatenate([y_tr, y_tr],               axis=0)
        model = Sequential([
            Input(shape=(seq_len, n_features)),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2), Dropout(0.3),
            Flatten(),
            Dense(64, activation='relu'), Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])
        loss_fn = 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'

    # c) 컴파일 & 콜백
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
    cbs = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # d) 학습
    model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=cbs, verbose=1
    )

    # e) 평가 & threshold 튜닝 (macro-F1)
    vt = np.argmax(y_va, axis=1)
    probs = model.predict(X_va)
    vp_arg = np.argmax(probs, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax F1(macro): {f1_arg:.4f}")

    if num_classes==2:
        best_f1, best_thr = f1_arg, 0.5
        if col == 'S3':
            thresh_grid = np.linspace(0.0, 1.0, 101)
        else:
            thresh_grid = np.linspace(0.1, 0.9, 17)
        for thr in thresh_grid:
             vp_thr = (probs[:,1] > thr).astype(int)
             f1_t = f1_score(vt, vp_thr, average='macro')
             if f1_t>best_f1:
                 best_f1, best_thr = f1_t, thr
        print(f"{col} best-thr={best_thr:.2f}, F1: {best_f1:.4f}")
        vp_final = (probs[:,1] > best_thr).astype(int) if best_f1>f1_arg else vp_arg
    else:
        vp_final = vp_arg
        print(f"{col} multiclass F1(macro): {f1_score(vt, vp_final, average='macro'):.4f}")

    print(classification_report(vt, vp_final, zero_division=0))
    models5_special[col] = model

"""model5. 나머지 Q1,2,3이랑 S2 최적 모델"""

from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Dropout,
    BatchNormalization, GlobalAveragePooling1D, Dense
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import f1_score, classification_report
import numpy as np

# ------------------------------------------
# (0) 시계열 Jitter 증강 함수
def augment_jitter(X, sigma=0.02):
    noise = np.random.normal(loc=0.0, scale=sigma, size=X.shape)
    return X + noise
# ------------------------------------------

models5 = {}
for col in ['Q1','Q2','Q3','S2']:
    print(f"\n===== Training for target: {col} =====")

    # (1) 레이블 준비
    idx        = target_cols.index(col)
    y_int      = Y[:, idx].astype(int)
    num_classes= len(np.unique(y_int))
    y_cat      = to_categorical(y_int, num_classes)

    # (2) Train/Val split
    train_idx, val_idx = split_idxs[col]
    X_train = X_scaled[train_idx].astype('float32')
    X_val   = X_scaled[val_idx].astype('float32')
    y_train = y_cat[ train_idx].astype('float32')
    y_val   = y_cat[ val_idx].astype('float32')

    # (3) 클래스 가중치
    cw_vals = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.arange(num_classes),
        y=y_int[train_idx]
    )
    cw = dict(enumerate(cw_vals))

    # (4) Jitter 증강
    X_aug = augment_jitter(X_train, sigma=0.02)
    y_aug = y_train.copy()
    X_train = np.concatenate([X_train, X_aug], axis=0)
    y_train = np.concatenate([y_train, y_aug], axis=0)

    # (5) 모델 정의
    model = Sequential([
        Input(shape=(seq_len, n_features)),
        Conv1D(64, 3, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.3),

        Conv1D(128, 5, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.4),

        Conv1D(256, 3, activation='relu'),
        BatchNormalization(), MaxPooling1D(2), Dropout(0.5),

        GlobalAveragePooling1D(),
        Dense(128, activation='relu'), Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    loss_fn = 'binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy'
    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

    # (6) 콜백
    callbacks = [
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(patience=3, factor=0.5)
    ]

    # (7) 학습
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        class_weight=cw,
        epochs=50, batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # (8) 평가 및 macro-F1 기반 임계치 선택
    vt        = np.argmax(y_val, axis=1)
    probs_all = model.predict(X_val)                  # shape (N, num_classes)

    # 8.1) argmax(.5) 기준
    vp_arg = np.argmax(probs_all, axis=1)
    f1_arg = f1_score(vt, vp_arg, average='macro')
    print(f"{col} argmax(.5) macro-F1: {f1_arg:.4f}")

    # 8.2) 이진 분류의 경우 threshold 튜닝
    if num_classes == 2:
        best_f1, best_thr = f1_arg, 0.5
        for thr in np.linspace(0.1, 0.9, 17):
            vp_thr = (probs_all[:,1] > thr).astype(int)
            f1_t = f1_score(vt, vp_thr, average='macro')
            if f1_t > best_f1:
                best_f1, best_thr = f1_t, thr

        print(f"{col} best-threshold={best_thr:.2f}, macro-F1: {best_f1:.4f}")

        # 최종 예측 선택
        if best_f1 > f1_arg:
            vp_final = (probs_all[:,1] > best_thr).astype(int)
            print(f"=> {col}: threshold={best_thr:.2f} 적용")
        else:
            vp_final = vp_arg
            print(f"=> {col}: argmax(.5) 유지")

    else:
        # 멀티클래스는 argmax 고정
        vp_final = vp_arg
        f1_m = f1_score(vt, vp_final, average='macro')
        print(f"{col} multiclass macro-F1: {f1_m:.4f}")

    # 8.3) 최종 리포트
    print(f"--- {col} 최종 성능 ---")
    print(classification_report(vt, vp_final, zero_division=0))

    models5[col] = model

# validation 예측 분포 확인
for col in target_cols:
    vp = np.argmax(models[col].predict(X_val), axis=1)
    unique, counts = np.unique(vp, return_counts=True)
    print(f"{col} val pred dist: {dict(zip(unique, counts))}")

"""model1,3,5,5_1 *앙상블*"""

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler

# ---------------------------------------
# 0) 설정
# ---------------------------------------
target_cols      = ['Q1','Q2','Q3','S1','S2','S3']
model5_1_targets = ['S1','S3']
best_threshold = {
    'Q1': 0.40, 'Q2': 0.45, 'Q3': 0.60,
    'S1': 0.50, 'S2': 0.50, 'S3': 0.60
}
W = {
    'models1':   np.array([1,1,1,2,1,1]),
    'models3':   np.array([1,1.5,1.5,1,1.5,1.5]),
    'models5':   np.array([1.2,1.2,1.2,1,1,1]),
    'models5_1': np.array([1,1,1,2,1,2]),
}

# ---------------------------------------
# 1) sample 불러오기 & 시퀀스 준비
# ---------------------------------------
submission = pd.read_csv('ch2025_submission_sample.csv')

# sid, 날짜 등 추출
submission['sid'] = (
    submission['subject_id'].astype(str)
      .str.extract(r'(\d+)', expand=False).astype(int)
)
submission['lifelog_date'] = pd.to_datetime(submission['lifelog_date']).dt.date
submission['sleep_date']   = pd.to_datetime(submission['sleep_date']).dt.date

# feature_cols, log_df 는 이전에 정의된 상태라고 가정
# (feature_cols = ['step','distance',…], log_df = 1m_merged.csv 전처리 결과)

submit_seqs = []
for _, row in submission.iterrows():
    grp = log_df[
        (log_df['subject_id']==row['sid']) &
        (log_df['date']==row['lifelog_date'])
    ]
    if grp.empty:
        seq = np.zeros((FIXED_LEN, len(feature_cols)), dtype='float32')
    else:
        seq = grp.sort_values('timestamp')[feature_cols].values
    submit_seqs.append(seq)

# ---------------------------------------
# 2) Padding & Scaling (X_test 준비)
# ---------------------------------------
FIXED_LEN = 1440
X_sub = pad_sequences(
    submit_seqs,
    maxlen=FIXED_LEN,
    padding='post', truncating='post', dtype='float32'
)
X_sub = np.nan_to_num(X_sub, nan=0., posinf=0., neginf=0.)

# scaler 는 train+val로 이미 fit 된 객체
n_features = X_sub.shape[2]
X_flat     = X_sub.reshape(-1, n_features)
X_flat_sc  = scaler.transform(X_flat)
X_test     = X_flat_sc.reshape(-1, FIXED_LEN, n_features)

# ---------------------------------------
# 3) 모델 확률 예측 모으기
# ---------------------------------------
# models1, models3, models5, models5_1 은 모두 메모리에 로드된 dict
P = {'models1':[], 'models3':[], 'models5':[], 'models5_1':[]}

for col in target_cols:
    # 모델별 예측
    P['models1'].append(models1[col].predict(X_test))
    P['models3'].append(models3[col].predict(X_test))

    if col in models5:
        P['models5'].append(models5[col].predict(X_test))
    else:
        C = P['models1'][-1].shape[1]
        P['models5'].append(np.ones((len(X_test),C))/C)

    if col in model5_1_targets:
        P['models5_1'].append(models5_1[col].predict(X_test))
    else:
        C = P['models1'][-1].shape[1]
        P['models5_1'].append(np.ones((len(X_test),C))/C)

# ---------------------------------------
# 4) Soft‐voting 앙상블
# ---------------------------------------
P_ens = []
for i, col in enumerate(target_cols):
    p1, p3 = P['models1'][i], P['models3'][i]
    p5      = P['models5'][i]
    p51     = P['models5_1'][i]
    w1,w3,w5,w51 = W['models1'][i], W['models3'][i], W['models5'][i], W['models5_1'][i]
    P_ens.append((w1*p1 + w3*p3 + w5*p5 + w51*p51)/(w1+w3+w5+w51))

# ---------------------------------------
# 5) Threshold/argmax → 예측값
# ---------------------------------------
preds = {}
for i, col in enumerate(target_cols):
    probs = P_ens[i]
    if probs.shape[1]==2:
        preds[col] = (probs[:,1] >= best_threshold[col]).astype(int)
    else:
        preds[col] = np.argmax(probs, axis=1)

# ---------------------------------------
# 6) 제출 파일 생성
# ---------------------------------------
subm = submission[['subject_id','sleep_date','lifelog_date']].copy()
for col in target_cols:
    subm[col] = preds[col]

subm.to_csv('submission_ensemble.csv', index=False)
print("✅ submission_ensemble.csv 생성 완료")

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score, accuracy_score
from tensorflow.keras.models import Sequential, load_model

# 0) 설정
target_cols      = ['Q1','Q2','Q3','S1','S2','S3']
model5_1_targets = ['S1','S3']
best_threshold   = {'Q1':0.40,'Q2':0.45,'Q3':0.60,'S1':0.50,'S2':0.50,'S3':0.60}
W = {
    'models1':   np.array([1,1,1,2,1,1]),
    'models3':   np.array([1,1.5,1.5,1,1.5,1.5]),
    'models5':   np.array([1.2,1.2,1.2,1,1,1]),
    'models5_1': np.array([1,1,1,2,1,2]),
}

# 1) Data + Preprocessing (train+val+test)
# -- assume you have X_seqs (list of arrays), Y_labels (list of 6-d vectors)
FIXED_LEN = 1440
X_pad = pad_sequences(X_seqs, maxlen=FIXED_LEN, padding='post', truncating='post', dtype='float32')
X_pad = np.nan_to_num(X_pad)
n_all, seq_len, n_features = X_pad.shape
scaler = StandardScaler()
X_flat = X_pad.reshape(-1, n_features)
X_scaled = scaler.fit_transform(X_flat).reshape(n_all, seq_len, n_features)
Y = np.vstack(Y_labels).astype(int)  # (n_all,6)

# 2) Train/Val split
split_idxs = {}
for i, col in enumerate(target_cols):
    idx = np.arange(n_all)
    tr, va = train_test_split(idx, test_size=0.2, stratify=Y[:,i], random_state=42)
    split_idxs[col] = (tr, va)

# 3) Prepare X_train, X_val, Y_train, Y_val
#    (모든 지표에 대해 동일한 train/val 인덱스를 쓰고 싶다면 stratify 기준만 바꿔주세요)
train_idx = np.unique(np.concatenate([v[0] for v in split_idxs.values()]))
val_idx   = np.unique(np.concatenate([v[1] for v in split_idxs.values()]))
X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
Y_train, Y_val = Y[train_idx], Y[val_idx]

# 4) Load or train your models
#    예: models1 = {col: load_model(...) for col in target_cols}
#    models3, models5, models5_1도 마찬가지로 준비했다고 가정

# 5) 함수로 평가하기
def evaluate_dict(models_dict, X, Y, name):
    print(f"\n---- {name} ----")
    for i, col in enumerate(target_cols):
        if col not in models_dict:
            print(f"[{col}] SKIP (no model)")
            continue

        probs = models_dict[col].predict(X)
        if probs.shape[1] == 2:
            preds = (probs[:,1] >= best_threshold[col]).astype(int)
        else:
            preds = np.argmax(probs, axis=1)

        y_true = Y[:, i]
        acc    = accuracy_score(y_true, preds)
        f1m    = f1_score(y_true, preds, average='macro')
        print(f"[{col}] Acc: {acc:.4f}, Macro-F1: {f1m:.4f}")

# 6) 단일 모델 평가
evaluate_dict(models1, X_train, Y_train, "models1 TRAIN")
evaluate_dict(models1, X_val,   Y_val,   "models1 VALID")
evaluate_dict(models3, X_train, Y_train, "models3 TRAIN")
evaluate_dict(models3, X_val,   Y_val,   "models3 VALID")
evaluate_dict(models5, X_train, Y_train, "models5 TRAIN")
evaluate_dict(models5, X_val,   Y_val,   "models5 VALID")
evaluate_dict(models5_1, X_train, Y_train, "models5_1 TRAIN")
evaluate_dict(models5_1, X_val,   Y_val,   "models5_1 VALID")

# 7) 앙상블 평가 (VALIDATION에 한해서)
def ensemble_predict(X):
    P = {'models1':[], 'models3':[], 'models5':[], 'models5_1':[]}
    for col in target_cols:
        P['models1'].append(models1[col].predict(X))
        P['models3'].append(models3[col].predict(X))
        P['models5'].append(models5[col].predict(X) if col in models5
                            else np.ones((len(X), P['models1'][-1].shape[1]))/P['models1'][-1].shape[1])
        P['models5_1'].append(models5_1[col].predict(X) if col in model5_1_targets
                              else np.ones((len(X), P['models1'][-1].shape[1]))/P['models1'][-1].shape[1])
    P_ens = []
    for i, col in enumerate(target_cols):
        p1, p3 = P['models1'][i], P['models3'][i]
        p5, p51 = P['models5'][i], P['models5_1'][i]
        w1,w3,w5,w51 = W['models1'][i], W['models3'][i], W['models5'][i], W['models5_1'][i]
        P_ens.append((w1*p1 + w3*p3 + w5*p5 + w51*p51)/(w1+w3+w5+w51))
    final = np.zeros_like(Y_val)
    for i, col in enumerate(target_cols):
        probs = P_ens[i]
        final[:,i] = (probs[:,1] >= best_threshold[col]).astype(int) if probs.shape[1]==2 else np.argmax(probs,axis=1)
    return final

y_ens_val = ensemble_predict(X_val)
print("\n---- ENSEMBLE VALID ----")
for i, col in enumerate(target_cols):
    print(f"{col}: Acc={accuracy_score(Y_val[:,i], y_ens_val[:,i]):.4f}, "
          f"Macro-F1={f1_score(Y_val[:,i], y_ens_val[:,i], average='macro'):.4f}")

"""편차가 크네 S1,S3 많이 올라온건 좋은데 Q1,2가 너무 떨어진거같아
지금까지 성능 좋았던 모델들 골라서 각 평가지표 예측해보겠다
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report

# — 0) 설정 —
target_cols = ['Q1','Q2','Q3','S1','S2','S3']
best_model = {
    'Q1':   ('models5',  'Q1'),
    'Q2':   ('models3',  'Q2'),
    'Q3':   ('models3',  'Q3'),
    'S1':   ('models1',  'S1'),
    'S2':   ('models3',  'S2'),
    'S3':   ('models3',  'S3'),
}
best_threshold = {
    'Q1': 0.40, 'Q2': 0.45, 'Q3': 0.60,
    'S1': 0.43, 'S2': 0.50, 'S3': 0.53
}
FIXED_LEN = 1440

# — padding/post-truncating 구현 함수 —
def np_pad_sequences(seqs, maxlen, n_features):
    N = len(seqs)
    X = np.zeros((N, maxlen, n_features), dtype='float32')
    for i, seq in enumerate(seqs):
        L = min(len(seq), maxlen)
        X[i, :L, :] = seq[:L]
    return X

# — 1) Train/Val 데이터 준비 (이전 코드와 동일한 전처리) —
#    X_seqs, Y_labels 는 “train+val” 리스트로 이미 준비돼 있다고 가정
#    feature_cols, log_df, scaler 도 이미 정의된 상태여야 합니다.

# 1-a) 패딩 → 스케일 → X_all
X_pad_all = np_pad_sequences(X_seqs, FIXED_LEN, len(feature_cols))
X_flat_all= X_pad_all.reshape(-1, len(feature_cols))
X_scaled_all = scaler.transform(X_flat_all).reshape(-1, FIXED_LEN, len(feature_cols))
Y_all = np.vstack(Y_labels).astype(int)  # shape (N_all, 6)

# 1-b) Train/Val split
from sklearn.model_selection import train_test_split
split_idxs = {}
for i, col in enumerate(target_cols):
    idx = np.arange(len(Y_all))
    tr, va = train_test_split(idx, test_size=0.2, stratify=Y_all[:,i], random_state=42)
    split_idxs[col] = (tr, va)

# 1-c) 공통 train/val 인덱스 (가장 넓게 union)
train_idx = np.unique(np.concatenate([v[0] for v in split_idxs.values()]))
val_idx   = np.unique(np.concatenate([v[1] for v in split_idxs.values()]))

X_train, X_val = X_scaled_all[train_idx], X_scaled_all[val_idx]
Y_train, Y_val = Y_all[train_idx],       Y_all[val_idx]

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ----------------------------
# 0) 설정된 변수 (이미 준비된 상태)
# ----------------------------
# - X_scaled: (N_all, seq_len, n_features) 전체 데이터
# - split_idxs: { col: (train_idx, val_idx) }
# - models1, models3, models5, models5_1: { col: trained_model }
# - scaler: StandardScaler (fit 완료)
# - log_df: 1m_merged 전처리 결과 DataFrame
# - feature_cols: 사용한 feature 리스트
# - ch2025_submission_sample.csv: sample 제출 파일

TARGET_COLS   = ['Q1','Q2','Q3','S1','S2','S3']
BEST_MODEL    = {
    'Q1': ('models5',  'Q1'),
    'Q2': ('models3',  'Q2'),
    'Q3': ('models3',  'Q3'),
    'S1': ('models1',  'S1'),
    'S2': ('models3',  'S2'),
    'S3': ('models3',  'S3'),
}
BEST_THRESHOLD =  {
    'Q1': 0.40, 'Q2': 0.45, 'Q3': 0.60,
    'S1': 0.43, 'S2': 0.50, 'S3': 0.53
}
FIXED_LEN = 1440

# ----------------------------
# 1) Train/Val 평가
# ----------------------------
def evaluate_per_target(models_dict, X, Y, name):
    print(f"\n==== {name} ====")
    for i, col in enumerate(TARGET_COLS):
        dict_name, mdl_col = BEST_MODEL[col]
        mdl = globals()[dict_name].get(mdl_col)
        if mdl is None:
            print(f"{col}: SKIP (no model)")
            continue

        probs = mdl.predict(X)
        if probs.shape[1] == 2:
            preds = (probs[:,1] >= BEST_THRESHOLD[col]).astype(int)
        else:
            preds = np.argmax(probs, axis=1)

        acc = accuracy_score(Y[:,i], preds)
        f1m = f1_score(Y[:,i], preds, average='macro')
        print(f"{col} → Acc: {acc:.4f}, Macro-F1: {f1m:.4f}")

# train/val 인덱스 통합
train_idx = np.unique(np.concatenate([v[0] for v in split_idxs.values()]))
val_idx   = np.unique(np.concatenate([v[1] for v in split_idxs.values()]))

X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
Y_all = np.vstack(Y_labels)  # (N_all, 6) 라벨
Y_train, Y_val = Y_all[train_idx], Y_all[val_idx]

# 평가 실행
evaluate_per_target(BEST_MODEL, X_train, Y_train, "PER-TARGET TRAIN")
evaluate_per_target(BEST_MODEL, X_val,   Y_val,   "PER-TARGET VALID")

# ----------------------------
# 2) 제출용 예측 & 파일 생성
# ----------------------------
# 2-1) Sample 및 시퀀스 로드
submission = pd.read_csv('ch2025_submission_sample.csv')
submission['sid'] = (
    submission['subject_id'].astype(str)
      .str.extract(r'(\d+)', expand=False)
      .astype(int)
)
submission['lifelog_date'] = pd.to_datetime(submission['lifelog_date']).dt.date
submission['sleep_date']   = pd.to_datetime(submission['sleep_date']).dt.date

seqs = []
for _, row in submission.iterrows():
    grp = log_df[
        (log_df['subject_id']==row['sid']) &
        (log_df['date']==row['lifelog_date'])
    ]
    if grp.empty:
        seqs.append(np.zeros((0, len(feature_cols)), dtype='float32'))
    else:
        seqs.append(grp.sort_values('timestamp')[feature_cols].values.astype('float32'))

# 2-2) Padding & Scaling
X_pad = pad_sequences(seqs, maxlen=FIXED_LEN,
                      padding='post', truncating='post', dtype='float32')
X_pad = np.nan_to_num(X_pad, nan=0., posinf=0., neginf=0.)
n_feat = X_pad.shape[2]
X_flat = X_pad.reshape(-1, n_feat)
X_test = scaler.transform(X_flat).reshape(-1, FIXED_LEN, n_feat)

# 2-3) Per-target 예측
preds = {}
for col in TARGET_COLS:
    dict_name, mdl_col = BEST_MODEL[col]
    mdl = globals()[dict_name][mdl_col]
    prob = mdl.predict(X_test)
    if prob.shape[1] == 2:
        preds[col] = (prob[:,1] >= BEST_THRESHOLD[col]).astype(int)
    else:
        preds[col] = np.argmax(prob, axis=1)

# 2-4) 제출 파일 작성
out = submission[['subject_id','sleep_date','lifelog_date']].copy()
for col in TARGET_COLS:
    out[col] = preds[col]
out.to_csv('submission_per_target.csv', index=False)
print("✅ submission_per_target.csv 생성 완료")

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score

# —————————————————————
# 0) 이미 준비된 변수
# —————————————————————
# X_train, Y_train, X_val, Y_val   : numpy arrays
# models1, models3, models5, models5_1: dicts of trained models
# BEST_MODEL    : per-target 모델 매핑 dict
# BEST_THRESHOLD: 기존 임계치 dict

TARGET_COLS = ['Q1','Q2','Q3','S1','S2','S3']

# —————————————————————
# 1) 5-Fold CV로 이진 타깃 threshold 재튜닝
# —————————————————————
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
new_thresholds = {}

for i, col in enumerate(TARGET_COLS):
    dict_name, mdl_col = BEST_MODEL[col]
    mdl = globals()[dict_name][mdl_col]

    # 이진 분류(target 클래스 수가 2)만 처리
    if mdl.output_shape[-1] != 2:
        continue

    y = Y_train[:, i]
    thr_list = []

    # reshape for stratification
    N, seq_len, n_feat = X_train.shape
    X_flat = X_train.reshape(N, -1)

    for tr_idx, va_idx in skf.split(X_flat, y):
        probs_va = mdl.predict(X_train[va_idx])[:, 1]
        best_thr, best_f1 = 0.5, 0.0

        for thr in np.linspace(0.1, 0.9, 17):
            preds = (probs_va >= thr).astype(int)
            f1 = f1_score(y[va_idx], preds, average='macro')
            if f1 > best_f1:
                best_f1, best_thr = f1, thr

        thr_list.append(best_thr)

    new_thresholds[col] = float(np.mean(thr_list))

print("▶ CV-tuned thresholds:", new_thresholds)

# —————————————————————
# 2) BEST_THRESHOLD 업데이트
# —————————————————————
BEST_THRESHOLD.update(new_thresholds)

# —————————————————————
# 3) validation에서 per-target 재평가
# —————————————————————
print("\n==== PER-TARGET VALID (CV-tuned) ====")
rows = []
for i, col in enumerate(TARGET_COLS):
    dict_name, mdl_col = BEST_MODEL[col]
    mdl = globals()[dict_name][mdl_col]
    probs = mdl.predict(X_val)

    if probs.shape[1] == 2:
        preds = (probs[:,1] >= BEST_THRESHOLD[col]).astype(int)
    else:
        preds = np.argmax(probs, axis=1)

    acc = accuracy_score(Y_val[:, i], preds)
    f1m = f1_score(Y_val[:, i], preds, average='macro')
    rows.append((col, acc, f1m))
    print(f"{col} → Acc: {acc:.4f}, Macro-F1: {f1m:.4f}")

# —————————————————————
# 4) 요약 테이블
# —————————————————————
df_results = pd.DataFrame(rows, columns=['Target','Accuracy','Macro-F1'])
print("\nSummary:\n", df_results)

