# -*- coding: utf-8 -*-
"""2025-1ë©”ì¸ ëª¨ë¸8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HxsQF7iAYinPSqFng25bjEjkmsO44YFv
"""

import torch

print(torch.__version__)                 # ì˜ˆ: '2.1.0+cu121'
print(torch.cuda.is_available())        # âœ… Trueê°€ ë– ì•¼ ì •ìƒ
print(torch.cuda.get_device_name(0))    # ì˜ˆ: 'Tesla T4'

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

from google.colab import drive
drive.mount('/content/drive')

!pip install albumentations

import os
import re
import csv
import random
from glob import glob

import numpy as np
import pandas as pd

from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler
import torchvision.transforms as v2
from torchvision.datasets import ImageFolder
from torch.cuda import empty_cache

import albumentations as A
from albumentations.core.transforms_interface import ImageOnlyTransform
from albumentations.pytorch import ToTensorV2
import torchvision.transforms as T

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder

import timm
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from tqdm.cli import tqdm
import cv2
import warnings
warnings.filterwarnings('ignore')

def seed_everything(seed=42):
    random.seed(seed)  # Python ë‚´ì¥ random ëª¨ë“ˆ
    os.environ['PYTHONHASHSEED'] = str(seed)  # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    np.random.seed(seed)  # NumPy
    torch.manual_seed(seed)  # PyTorch CPU ì‹œë“œ ê³ ì •
    torch.cuda.manual_seed(seed)  # PyTorch GPU ì‹œë“œ ê³ ì •
    torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU í™˜ê²½ì—ì„œë„ ì‹œë“œ ê³ ì •
    torch.backends.cudnn.deterministic = True  # CuDNN ê´€ë ¨ ì„¤ì •
    torch.backends.cudnn.benchmark = False  # ë™ì¼í•œ ì…ë ¥ í¬ê¸°ì˜ ë°ì´í„°ê°€ ë°˜ë³µë  ê²½ìš° ì†ë„ í–¥ìƒì„ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ ë¹„í™œì„±í™”

# ì‚¬ìš© ì˜ˆì‹œ
seed_everything(seed=42)

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
csv_path = "/content/drive/MyDrive/dscover/2025-1ë©”ì¸/ch2025_metrics_train.csv"
image_root = "/content/drive/MyDrive/dscover/2025-1ë©”ì¸/images"
submission_path = "/content/drive/MyDrive/dscover/2025-1ë©”ì¸/ch2025_submission_sample.csv"
submission_df = pd.read_csv(submission_path)

df = pd.read_csv(csv_path)
df = df[~df['sleep_date'].isin(submission_df['sleep_date'])].reset_index(drop=True)
df['data_path'] = df.apply(
    lambda row: f"/content/drive/MyDrive/dscover/2025-1ë©”ì¸/images/"
                f"{row['subject_id']}_images/day_{row['sleep_date']}.png",
    axis=1
)

df.head()

train_df, valid_df = train_test_split(df, test_size = 0.2, random_state=1020)
train_df = train_df.sample(frac=1).reset_index(drop=True)

print(f"count >>> train_set: {len(train_df)}, valid_set: {len(valid_df)}")

# # ìƒìœ„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
# folder_path = '/content/drive/MyDrive/dscover/2025-1ë©”ì¸/images'

# # 2. ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ .png íŒŒì¼ ìˆ˜ì§‘
# image_files = []
# for root, dirs, files in os.walk(folder_path):
#     for file in files:
#         if file.endswith(".png"):
#             image_files.append(os.path.join(root, file))

# print(f"ì´ ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")

# # 3. ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ (ì˜ˆ: 1000ê°œ)
# sample_size = min(1000, len(image_files))
# sampled_files = random.sample(image_files, sample_size)

# # 4. ëˆ„ì  í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°
# n_pixels = 0
# sum_pixels = 0.0
# sum_squared_pixels = 0.0

# for file in sampled_files:
#     image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)
#     if image is not None:
#         pixels = image.astype(np.float32)
#         n_pixels += pixels.size
#         sum_pixels += np.sum(pixels)
#         sum_squared_pixels += np.sum(pixels ** 2)

# 5. í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
mean = 3.7926549911499023
std = 28.325054168701172

print(f"Mean: {mean}, Std: {std}")

class RandomHorizontalStrips(ImageOnlyTransform):
    def __init__(self, num_strips=(1, 3), strip_width=84, always_apply=False, p=0.5):
        super(RandomHorizontalStrips, self).__init__(always_apply, p)
        self.num_strips = num_strips
        self.strip_width = strip_width

    def apply(self, img, **params):
        h, w = img.shape[:2]
        num_strips = np.random.randint(self.num_strips[0], self.num_strips[1] + 1)
        for _ in range(num_strips):
            x_start = np.random.randint(0, w - self.strip_width)
            img[:, x_start:x_start + self.strip_width] = 0
        return img

    def get_transform_init_args_names(self):
        return ("num_strips", "strip_width")

    def get_params(self):
        return {"num_strips": self.num_strips, "strip_width": self.strip_width}

mean, std = 0.5, 0.5  # í‘ë°± ì´ë¯¸ì§€ì´ë¯€ë¡œ ë‹¨ì¼ ê°’ ì‚¬ìš©

train_transforms = A.Compose([
    A.ToGray(p=1.0),                         # RGB â†’ Grayscale
    RandomHorizontalStrips(p=0.5),          # ê°€ë¡œ ìŠ¤íŠ¸ë¦½ ì¶”ê°€
    A.Resize(height=224, width=224, p=1),
    A.GaussNoise(p=0.5),
    A.OneOf([
        A.GaussianBlur(p=0.5),
        A.Sharpen(p=0.5)
    ], p=0.5),
    A.Normalize(mean=[mean], std=[std], p=1.0),  # âš ï¸ 1ì±„ë„ë¡œ ì„¤ì •
    ToTensorV2()
])

test_transforms = A.Compose([
    A.ToGray(p=1.0),
    A.Resize(always_apply=True, height=224, width=224),
    A.Normalize(mean=[mean], std=[std], p=1.0),
    ToTensorV2()
])

# import torch.nn.functional as F
# from torch.utils.data import random_split

# # 1. ë³€í™˜ ì •ì˜ (í•™ìŠµìš©/ê²€ì¦ìš© ëª¨ë‘ ë™ì¼í•˜ê²Œ grayscale + resize + tensor ë³€í™˜ë§Œ)
# transform = T.Compose([
#     T.Grayscale(),
#     T.Resize((224, 224)),
#     T.ToTensor(),
# ])

# # 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° Train/Val ë¶„ë¦¬ (ëˆ„ìˆ˜ ë°©ì§€ í•µì‹¬)
# root_dir = '/content/drive/MyDrive/dscover/2025-1ë©”ì¸/images'
# full_dataset = ImageFolder(root=root_dir, transform=transform)

# val_ratio = 0.2
# val_size = int(len(full_dataset) * val_ratio)
# train_size = len(full_dataset) - val_size

# train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))

# train_loader = DataLoader(train_dataset, batch_size= 16, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size= 16, shuffle=False)

# class GrayVAE(nn.Module):
#     def __init__(self, latent_dim=64):
#         super().__init__()
#         self.encoder = nn.Sequential(
#             nn.Conv2d(1, 32, 4, 2, 1),  # [B, 32, 112, 112]
#             nn.ReLU(),
#             nn.Conv2d(32, 64, 4, 2, 1), # [B, 64, 56, 56]
#             nn.ReLU(),
#             nn.Conv2d(64, 128, 4, 2, 1),# [B, 128, 28, 28]
#             nn.ReLU(),
#             nn.Flatten()
#         )
#         self.fc_mu = nn.Linear(128*28*28, latent_dim)
#         self.fc_logvar = nn.Linear(128*28*28, latent_dim)

#         self.decoder_fc = nn.Linear(latent_dim, 128*28*28)
#         self.decoder = nn.Sequential(
#             nn.Unflatten(1, (128, 28, 28)),
#             nn.ConvTranspose2d(128, 64, 4, 2, 1), # [B, 64, 56, 56]
#             nn.ReLU(),
#             nn.ConvTranspose2d(64, 32, 4, 2, 1),  # [B, 32, 112, 112]
#             nn.ReLU(),
#             nn.ConvTranspose2d(32, 1, 4, 2, 1),   # [B, 1, 224, 224]
#             nn.Sigmoid()
#         )

#     def encode(self, x):
#         h = self.encoder(x)
#         return self.fc_mu(h), self.fc_logvar(h)

#     def reparameterize(self, mu, logvar):
#         std = torch.exp(0.5 * logvar)
#         eps = torch.randn_like(std)
#         return mu + eps * std

#     def decode(self, z):
#         h = self.decoder_fc(z)
#         return self.decoder(h)

#     def forward(self, x):
#         mu, logvar = self.encode(x)
#         z = self.reparameterize(mu, logvar)
#         x_recon = self.decode(z)
#         return x_recon, mu, logvar

# # 4. VAE ì†ì‹¤ í•¨ìˆ˜
# def vae_loss_function(recon_x, x, mu, logvar):
#     recon_loss = F.mse_loss(recon_x, x, reduction='mean')  # í‰ê·  ê¸°ë°˜
#     kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
#     return recon_loss + kl_div

# # 5. í•™ìŠµ ë£¨í”„ (ê²€ì¦ í¬í•¨)
# device = 'cuda' if torch.cuda.is_available() else 'cpu'
# vae = GrayVAE(latent_dim=64).to(device)
# optimizer = torch.optim.Adam(vae.parameters(), lr=1e-4)

# epochs = 10
# for epoch in range(epochs):
#     vae.train()
#     train_loss = 0
#     for x, _ in train_loader:
#         x = x.to(device)
#         recon_x, mu, logvar = vae(x)
#         loss = vae_loss_function(recon_x, x, mu, logvar)

#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * x.size(0)

#     # ê²€ì¦
#     vae.eval()
#     val_loss = 0
#     with torch.no_grad():
#         for x, _ in val_loader:
#             x = x.to(device)
#             recon_x, mu, logvar = vae(x)
#             loss = vae_loss_function(recon_x, x, mu, logvar)
#             val_loss += loss.item() * x.size(0)

#     print(f"[Epoch {epoch+1}] Train Loss: {train_loss/len(train_loader.dataset):.4f} | Val Loss: {val_loss/len(val_loader.dataset):.4f}")

# # 6. ëª¨ë¸ ì €ì¥ (ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ í•™ìŠµ ì „ìš© ëª¨ë¸ë§Œ ì €ì¥)
# torch.save(vae.state_dict(), 'vae_gray_pretrained2.pt')

# save_path = "/content/drive/MyDrive/dscover/2025-1ë©”ì¸/vae_gray_pretrained2.pt"
# torch.save(vae.state_dict(), save_path)

# VAE ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
vae = GrayVAE(latent_dim=64).to('cuda')
vae.load_state_dict(torch.load('/content/drive/MyDrive/dscover/2025-1ë©”ì¸/vae_gray_pretrained2.pt'))
vae.eval()

from torchvision.transforms import ToPILImage
from albumentations.pytorch import ToTensorV2
import albumentations as A
import numpy as np
import cv2
import torch

def vae_augment(images, vae, noise_std=0.1):
    # ëª¨ë¸ì€ í•­ìƒ í‰ê°€ ëª¨ë“œë¡œ (Dropout/BN ë“± off)
    vae.eval()

    # ì±„ë„/í¬ê¸° ëª¨ë‘ ë§ì¶°ì•¼ ì•ˆì „
    if images.shape[1:] != (1, 224, 224):
        images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)

    with torch.no_grad():  # âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì  ë°©ì§€
        recon, mu, logvar = vae(images)

    # ë…¸ì´ì¦ˆ ì¶”ê°€ (ì˜µì…˜ì„± augmentation)
    if noise_std > 0:
        noise = torch.randn_like(recon) * noise_std
        recon = recon + noise
        recon = torch.clamp(recon, 0.0, 1.0)

    return recon

class CustomDataset(Dataset):
    def __init__(self, dataframe, transform=None, is_test=False,
                 use_vae_aug=False, vae=None, noise_std=0.1):
        self.df = dataframe
        self.transform = transform
        self.is_test = is_test
        self.use_vae_aug = use_vae_aug and not is_test  # âœ… testì—” VAE ì¦ê°• ê¸ˆì§€
        self.vae = vae
        self.noise_std = noise_std

        if self.use_vae_aug and self.vae is not None:
            self.vae.eval()  # âœ… Dropout/BN ë¹„í™œì„±í™”

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['data_path']

        # âœ… ì´ë¯¸ì§€ ë¡œë”© (í‘ë°± + 1ì±„ë„ë¡œ í™•ì¥)
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = np.expand_dims(image, axis=-1)  # (H, W, 1)

        # âœ… transform ì ìš© (ToTensor í¬í•¨)
        if self.transform:
            image = self.transform(image=image)['image']  # torch.Tensor (1, H, W)

        # âœ… VAE ê¸°ë°˜ ì¦ê°• (trainë§Œ, tensor ìƒíƒœì—ì„œë§Œ)
        if self.use_vae_aug and self.vae is not None and isinstance(image, torch.Tensor):
            image = vae_augment(image.unsqueeze(0), self.vae, noise_std=self.noise_std)[0]

        # âœ… ë°˜í™˜
        if self.is_test:
            return image
        else:
            label = torch.tensor(row[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].astype(np.int64).values, dtype=torch.long)
            return image, label

train_dataset = CustomDataset(df, transform=train_transforms, use_vae_aug=True, vae=vae)
valid_dataset = CustomDataset(valid_df, test_transforms,  use_vae_aug=False)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)

epochs = 20

from collections import deque

def run_model(model, loader, loss_fns, optimizer=None, is_training=False, epoch=None, device='cuda'):
    num_labels = len(loss_fns)
    all_targets = [[] for _ in range(num_labels)]
    all_preds = [[] for _ in range(num_labels)]
    smooth_loss_queue = deque(maxlen=50)

    if is_training:
        model.train()
    else:
        model.eval()

    mode = 'Train' if is_training else 'Valid/Test'
    running_loss = 0.0

    bar = tqdm(loader, ascii=True, leave=False)

    context = torch.enable_grad() if is_training else torch.no_grad()  # âœ… ëˆ„ìˆ˜ ë°©ì§€ í•µì‹¬
    with context:
        for cnt, (data, target) in enumerate(bar):
            data = data.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)

            if is_training:
                optimizer.zero_grad()

            outputs = model(data)  # âœ… forward

            losses = []
            for i in range(num_labels):
                out_i = outputs[i]               # [B, C_i]
                target_i = target[:, i].long()   # [B]

                loss_i = loss_fns[i](out_i, target_i)
                losses.append(loss_i)

                pred_i = torch.argmax(out_i, dim=1)
                all_preds[i].extend(pred_i.cpu().tolist())
                all_targets[i].extend(target_i.cpu().tolist())

            total_loss = sum(losses)
            if is_training:
                total_loss.backward()
                optimizer.step()

            loss_val = total_loss.item()
            running_loss += loss_val
            smooth_loss_queue.append(loss_val)
            smooth_loss = sum(smooth_loss_queue) / len(smooth_loss_queue)

            bar.set_description(f"Epoch {epoch if epoch is not None else '?'} {mode}")
            bar.set_postfix(loss=f"{loss_val:.4f}", smooth=f"{smooth_loss:.4f}")

            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
            del data, target, outputs, total_loss, losses

    # ë©”íŠ¸ë¦­ ê³„ì‚° (ëˆ„ìˆ˜ ì—†ìŒ)
    f1s = [f1_score(all_targets[i], all_preds[i], average='macro') for i in range(num_labels)]
    accs = [accuracy_score(all_targets[i], all_preds[i]) for i in range(num_labels)]

    avg_f1 = sum(f1s) / num_labels
    avg_acc = sum(accs) / num_labels
    return running_loss / len(loader), avg_acc, avg_f1

class MultiHeadResNet(nn.Module):
    def __init__(self, head_classes=[2, 2, 2, 3, 2, 2], pretrained=False):  # âœ… pretrained=False
        super().__init__()
        self.backbone = timm.create_model(
            'resnet18',
            pretrained=pretrained,     # âœ… í‘ë°± ë„ë©”ì¸ì— pretrained=TrueëŠ” ìœ„í—˜
            in_chans=1,                # âœ… 1ì±„ë„ ì…ë ¥ (gray)
            num_classes=0,             # âœ… head ì œê±°
            global_pool='avg'          # âœ… ì•ˆì •ì ì¸ global pooling
        )
        in_features = self.backbone.num_features

        self.heads = nn.ModuleList([
            nn.Linear(in_features, c) for c in head_classes
        ])

    def forward(self, x):
        feats = self.backbone(x)  # [B, in_features]
        return [head(feats) for head in self.heads]

import math
from torch.optim.lr_scheduler import _LRScheduler

class CosineAnnealingWarmUpRestarts(_LRScheduler):
    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1.0, last_epoch=-1):
        if not isinstance(T_0, int) or T_0 <= 0:
            raise ValueError("T_0 must be a positive integer")
        if not isinstance(T_mult, int) or T_mult < 1:
            raise ValueError("T_mult must be an integer >= 1")
        if not isinstance(T_up, int) or T_up < 0:
            raise ValueError("T_up must be a non-negative integer")

        self.T_0 = T_0
        self.T_mult = T_mult
        self.T_up = T_up
        self.eta_max = eta_max
        self.base_eta_max = eta_max  # í•­ìƒ decay ì „ ê¸°ì¤€ê°’ ë³´ê´€
        self.gamma = gamma

        self.T_i = T_0
        self.cycle = 0
        self.T_cur = last_epoch

        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.T_cur == -1:
            # ì´ˆê¸° ìƒíƒœì—ì„œë„ base_lr ë°˜í™˜ â†’ ì¼ê´€ì„±
            return [base_lr for base_lr in self.base_lrs]

        lrs = []
        for base_lr in self.base_lrs:
            if self.T_cur < self.T_up:
                # Warm-up ë‹¨ê³„: ì„ í˜• ì¦ê°€
                lr = base_lr + (self.eta_max - base_lr) * self.T_cur / max(1, self.T_up)
            else:
                # Cosine decay
                progress = (self.T_cur - self.T_up) / max(1, self.T_i - self.T_up)
                cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
                lr = base_lr + (self.eta_max - base_lr) * cosine_decay
            lrs.append(lr)
        return lrs

    def step(self, epoch=None):
        if epoch is None:
            self.T_cur += 1
            if self.T_cur >= self.T_i:
                self.cycle += 1
                self.T_cur = self.T_cur - self.T_i
                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up
        else:
            if epoch < self.T_0:
                self.cycle = 0
                self.T_i = self.T_0
                self.T_cur = epoch
            else:
                if self.T_mult == 1:
                    self.cycle = epoch // self.T_0
                    self.T_i = self.T_0
                    self.T_cur = epoch % self.T_0
                else:
                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))
                    self.cycle = n
                    self.T_i = self.T_0 * self.T_mult ** n
                    self.T_cur = epoch - int(self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1))

        # Decay eta_max (ë”°ë¼ì„œ cosine ì§„í­ë„ ì ì  ì¤„ì–´ë“¦)
        self.eta_max = self.base_eta_max * (self.gamma ** self.cycle)

        self.last_epoch = int(epoch if epoch is not None else self.last_epoch + 1)

        # ì‹¤ì œ í•™ìŠµë¥  ì ìš©
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

from torch.utils.data import TensorDataset

head_classes = [2, 2, 2, 3, 2, 2]  # Q1~S3
vae = GrayVAE().to('cuda')
vae.eval()

submission_sample = pd.read_csv("/content/drive/MyDrive/dscover/2025-1ë©”ì¸/ch2025_submission_sample.csv")
submission_pairs = set(
    zip(submission_sample['subject_id'], submission_sample['sleep_date'])
)

# â‘¡ trainì—ì„œ í•´ë‹¹ ë‚ ì§œ ì œê±°
df_filtered = df[~df[['subject_id', 'sleep_date']].apply(tuple, axis=1).isin(submission_pairs)].reset_index(drop=True)

# â‘¢ í•„í„°ë§ëœ dfë¡œ CustomDataset ìƒì„±
full_dataset = CustomDataset(df_filtered, train_transforms)

k_folds = 3
kf = KFold(n_splits=k_folds, shuffle=True, random_state=1020)
fold_perf = {}

for fold, (train_idx, valid_idx) in enumerate(kf.split(full_dataset)):
    print(f"\n[Fold {fold+1}]")

    train_loader = DataLoader(
        full_dataset, batch_size=16, sampler=SubsetRandomSampler(train_idx),
        num_workers=0, pin_memory=True
    )
    valid_loader = DataLoader(
        full_dataset, batch_size=16, sampler=SubsetRandomSampler(valid_idx),
        num_workers=0, pin_memory=True
    )

    model = MultiHeadResNet(head_classes=head_classes).to('cuda')
    # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ê¸°ë°˜ weight (ë¹„ìœ¨ ì‘ì„ìˆ˜ë¡ weight í¬ê²Œ)
    weight_q1 = torch.tensor([0.37, 0.63], dtype=torch.float32)
    weight_q2 = torch.tensor([0.50, 0.50], dtype=torch.float32)
    weight_q3 = torch.tensor([0.49, 0.51], dtype=torch.float32)
    weight_s1 = torch.tensor([0.29, 0.16, 0.54], dtype=torch.float32)
    weight_s2 = torch.tensor([0.26, 0.74], dtype=torch.float32)
    weight_s3 = torch.tensor([0.26, 0.74], dtype=torch.float32)

    # GPUì— ì˜¬ë¦¬ê¸°
    weight_q1 = weight_q1.to('cuda')
    weight_q2 = weight_q2.to('cuda')
    weight_q3 = weight_q3.to('cuda')
    weight_s1 = weight_s1.to('cuda')
    weight_s2 = weight_s2.to('cuda')
    weight_s3 = weight_s3.to('cuda')

    # ğŸ¯ ê° targetë³„ë¡œ ê°€ì¤‘ì¹˜ ì ìš©ëœ criterion ìƒì„±
    criterion = [
        nn.CrossEntropyLoss(weight=weight_q1),  # Q1
        nn.CrossEntropyLoss(weight=weight_q2),  # Q2
        nn.CrossEntropyLoss(weight=weight_q3),  # Q3
        nn.CrossEntropyLoss(weight=weight_s1),  # S1
        nn.CrossEntropyLoss(weight=weight_s2),  # S2
        nn.CrossEntropyLoss(weight=weight_s3),  # S3
    ]

    optimizer = optim.AdamW(model.parameters(), lr=5e-4)
    scheduler = CosineAnnealingWarmUpRestarts(
        optimizer, T_0=10, T_mult=2, eta_max=5e-5, T_up=3, gamma=0.5
    )

    best_f1, best_loss = -float('inf'), float('inf')
    best_model_state = None

    for epoch in range(epochs):
        model.train()
        all_train_images = []
        all_train_labels = []

        for images, labels in train_loader:
            images = images.to('cuda')
            labels = labels.to('cuda')
            gray_images = images.mean(dim=1, keepdim=True) # [B, 3, H, W] â†’ [B, 1, H, W]
            aug_images = vae_augment(gray_images, vae, noise_std=0.1)

            combined_images = torch.cat([images, aug_images], dim=0)
            combined_labels = torch.cat([labels, labels], dim=0)

            all_train_images.append(combined_images)
            all_train_labels.append(combined_labels)

        full_inputs = torch.cat(all_train_images, dim=0)
        full_targets = torch.cat(all_train_labels, dim=0)

        train_dataset = TensorDataset(full_inputs, full_targets)
        train_loader_aug = DataLoader(train_dataset, batch_size=16, shuffle=True)

        # í•™ìŠµ
        train_loss, train_acc, train_f1 = run_model(
            model, train_loader_aug, criterion, optimizer,
            is_training=True, epoch=epoch
        )

        # ê²€ì¦
        with torch.no_grad():
            valid_loss, valid_acc, valid_f1 = run_model(
                model, valid_loader, criterion, optimizer,
                is_training=False, epoch=epoch
            )

        print(
            f"Epoch {epoch+1} | "
            f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | "
            f"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1:.4f} | "
            f"LR: {optimizer.param_groups[0]['lr']:.2e}"
        )

        if valid_f1 > best_f1:
            best_f1 = valid_f1
            best_loss = valid_loss
            best_model_state = model.state_dict()
            print(f"  ğŸ”¥ New best model saved for fold {fold+1}")

        scheduler.step()
        empty_cache()

    model_path = f'model_fold_{fold}_f1-{best_f1:.3f}_loss-{best_loss:.3f}.pt'
    torch.save(best_model_state, model_path)
    print(f"âœ… Saved: {model_path}")

    fold_perf[fold] = {
        'train_loss': train_loss,
        'train_acc': train_acc,
        'train_f1': train_f1,
        'valid_loss': valid_loss,
        'valid_acc': valid_acc,
        'valid_f1': valid_f1
    }

    del model, optimizer, scheduler, best_model_state
    empty_cache()

from datetime import timedelta
from pathlib import Path
# ------------------ ê²½ë¡œ ì„¤ì • ------------------
image_root = "/content/drive/MyDrive/dscover/2025-1ë©”ì¸/images"
target_cols = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']
head_classes = [2, 2, 2, 3, 2, 2]

# âœ… sleep_date ê¸°ì¤€ ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸° (ì˜¤ëŠ˜ â†’ 7ì¼ ì „ â†’ 7ì¼ í›„)
def find_valid_image(row):
    base_date = pd.to_datetime(row['sleep_date'])
    for offset in [0, -7, 7]:
        try_date = (base_date + timedelta(days=offset)).strftime('%Y-%m-%d')
        path = f"{image_root}/{row['subject_id']}_images/day_{try_date}.png"
        if Path(path).exists():
            return path
    return None

# ğŸ” ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„±
submission_df['data_path'] = submission_df.apply(find_valid_image, axis=1)

# âœ… ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ëŠ” í–‰ë§Œ ì˜ˆì¸¡ ëŒ€ìƒìœ¼ë¡œ ì¶”ì¶œ
has_image_mask = submission_df['data_path'].notna()
image_df = submission_df[has_image_mask].copy()

# âœ… DataLoader ì¤€ë¹„
image_dataset = CustomDataset(image_df, transform=test_transforms, is_test=True)
image_loader = DataLoader(image_dataset, batch_size=16, shuffle=False)

# âœ… ëª¨ë¸ ë¡œë”©
model = MultiHeadResNet(head_classes).to('cuda')
model.load_state_dict(torch.load('model_fold_2_f1-0.466_loss-4.066.pt'))
model.eval()

# âœ… ì˜ˆì¸¡ ìˆ˜í–‰
preds_by_col = [[] for _ in range(6)]
with torch.no_grad():
    for images in tqdm(image_loader, desc="ğŸ“Š Predicting available images"):
        images = images.to('cuda')
        outputs = model(images)
        for i in range(6):
            preds = torch.argmax(outputs[i], dim=1).cpu().tolist()
            preds_by_col[i].extend(preds)

# âœ… ì˜ˆì¸¡ê°’ image_dfì— ì €ì¥
for i, col in enumerate(target_cols):
    image_df[col] = preds_by_col[i]

# âœ… ì›ë³¸ submission_dfì— ë°˜ì˜ (ì´ë¯¸ì§€ ê²½ë¡œê°€ ìˆëŠ” í–‰ë§Œ)
submission_df.update(image_df[target_cols])

# âœ… ID í¬ë§· ì •ë¦¬
submission_df['subject_id'] = submission_df['subject_id'].apply(
    lambda x: f"id{int(str(x).replace('id', '')):02d}"
)

# âœ… ê²°ê³¼ ì €ì¥
final_submission = submission_df[['subject_id', 'sleep_date', 'lifelog_date'] + target_cols]
final_submission.to_csv("/content/drive/MyDrive/dscover/2025-1ë©”ì¸/submission11.csv", index=False)
print("âœ… ì œì¶œ ì™„ë£Œ: submission11.csv ì €ì¥ë¨")

print(pd.Series(predictions[0]).value_counts())

# ëŒ€ìƒ ì»¬ëŸ¼
target_cols = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']

# ê° í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ê³„ì‚°
for col in target_cols:
    print(f"\nğŸ“Š {col} í´ë˜ìŠ¤ ë¹„ìœ¨:")
    class_counts = df[col].value_counts(normalize=True, dropna=True).sort_index()
    for cls, ratio in class_counts.items():
        print(f"  í´ë˜ìŠ¤ {cls}: {ratio * 100:.2f}%")